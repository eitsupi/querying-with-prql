---
title: Reshaping and Tidy Data
description: Make the data tidy.
engine: knitr
knitr:
  opts_chunk:
    connection: con
    engine-opts:
      target: sql.duckdb
      use_glue: true
      info_string: '{.sql filename="SQL"}'
execute:
  cache: true
sidebar_position: 4
---

:::{.callout-note}

This page is based on the chapter ["Reshaping and Tidy Data"](https://kevinheavey.github.io/modern-polars/tidy.html)
of the Modern Polars book.

:::

## Read the Data

{{< include _setup-knitr.qmd >}}

### Download

Download the data to be analysis (tables on the website) and write the data to CSV files.

This document uses R to download the data from the source here,
but we can also download and use the CSV files included in
the [kevinheavey/modern-polars](https://github.com/kevinheavey/modern-polars/tree/master/data/nba) GitHub repository.

:::{.panel-tabset}

#### R

```{r}
#| filename: R
#| code-fold: true
#| cache: false
nba_dir <- file.path("data", "nba")

months <- c(
  "october",
  "november",
  "december",
  "january",
  "february",
  "march",
  "april",
  "may",
  "june"
)

column_names <- c(
  date = "date",
  away_team = "visitor_neutral",
  away_points = "pts",
  home_team = "home_neutral",
  home_points = "pts_2"
)

.write_data <- function(month) {
  base_url <- "http://www.basketball-reference.com/leagues/NBA_2016_games-{month}.html"

  glue::glue(base_url, month = month) |>
    rvest::read_html() |>
    rvest::html_table() |>
    (\(x) x[[1]])() |> # TODO: Rewrite after R 4.3
    janitor::clean_names() |>
    dplyr::select(all_of(column_names)) |>
    dplyr::filter(date != "Playoffs") |>
    readr::write_csv(file.path(nba_dir, glue::glue("{month}.csv")))
}

if (!fs::dir_exists(nba_dir)) {
  fs::dir_create(nba_dir)
  months |>
    purrr::walk(.write_data)
}
```

#### Shell

This is a sample command to download the CSV files from the `kevinheavey/modern-polars` GitHub repository.

```{r}
#| results: asis
#| echo: false
base_command <- glue::glue("curl -sL https://github.com/kevinheavey/modern-polars/raw/87539190dde3e99d5e4c4f9957c78932a33075a0/data/nba/{{month}}.csv -o {nba_dir}/{{month}}.csv")

commands <- glue::glue(base_command, month = months) |>
  stringr::str_c(collapse = "\n")

cat(
  "```{.bash filename=Terminal}",
  glue::glue("mkdir {nba_dir}"),
  commands,
  "```",
  sep = "\n"
)
```

:::

### Load the Data

After the CSV files are ready, load these into DuckDB (in-memory) database table,
R DataFrame, and Python polars.LazyFrame.

:::{.panel-tabset}

#### DuckDB

```{r }
#| filename: R
#| cache: false
#| output: false
#| echo: false
con <- DBI::dbConnect(duckdb::duckdb(), ":memory:")
```

```{glue_sql}
#| filename: SQL
#| cache: false
#| output: false
CREATE TABLE tab AS FROM read_csv_auto('data/nba/*.csv')
```

```{glue_sql}
#| filename: SQL
FROM tab
LIMIT 5
```

#### R DataFrame

```{r}
#| filename: R
#| cache: false
library(dplyr, warn.conflicts = FALSE)

df <- readr::read_csv(
  fs::dir_ls("data/nba", glob = "*.csv")
)
```

```{r}
#| filename: R
df |> head(5)
```

#### Python polars.LazyFrame

```{python}
#| filename: Python
#| cache: false
import polars as pl

lf = pl.scan_csv("data/nba/*.csv")
```

```{python}
#| filename: Python
lf.head(5).collect()
```

:::

## Cleaning {#sec-cleaning}

Convert the `date` column to date type and delete rows containing missing values (`null`).

PRQL does not have a "remove rows with missing values in any column" syntax
([PRQL/prql#2386](https://github.com/PRQL/prql/issues/2386)),
but DuckDB SQL does (>= 0.8, [duckdb/duckdb#6621](https://github.com/duckdb/duckdb/pull/6621)), so it can be used.

:::{.panel-tabset}

### PRQL DuckDB

```{prql}
#| filename: PRQL
#| label: prql_cleaning
let games = (
  from tab
  filter s"COLUMNS(*) IS NOT NULL"
  derive date_new = (s"strptime(date, '%a, %b %d, %Y')" | as date)
  select !{this.date} # `this` points to refer to current relation
  sort date_new
  derive game_id = (row_number this)
)

from games
take 5
```

### SQL DuckDB

```{sql}
--| filename: SQL
--| cache: false
--| output: false
CREATE TABLE games AS (
  WITH _tab1 AS (
      SELECT
        * REPLACE (strptime(date, '%a, %b %d, %Y')::date AS date)
      FROM tab
      WHERE COLUMNS(*) IS NOT NULL
    )

  SELECT
    row_number() OVER(ORDER BY date) AS game_id,
    *
  FROM _tab1
  ORDER BY date
)
```

```{glue_sql}
#| filename: SQL
FROM games
LIMIT 5
```

### dplyr R

```{r}
#| filename: R
games <- df |>
  filter(!if_any(everything(), is.na)) |> # Also can use `tidyr::drop_na`
  mutate(
    date = lubridate::parse_date_time(date, "%a, %b %d, %Y") |>
      lubridate::as_date()
  ) |>
  arrange(date) |>
  mutate(game_id = row_number(), .before = 1)
```

```{r}
#| filename: R
games |>
  head(5)
```

### Python Polars

```{python}
#| filename: Python
#| cache: false
games = (
    lf.filter(~pl.any_horizontal(pl.all().is_null())) # Also can use `polars.LazyFrame.drop_nulls`
    .with_columns(
        pl.col("date").str.strptime(pl.Date, "%a, %b %d, %Y"),
    )
    .sort("date")
    .with_row_index("game_id")
    .collect()
)
```

```{python}
#| filename: Python
games.head(5)
```

:::

Looking at the result tables, we notice that the PRQL result is different from the other results;
A column named `date` in other results is named `date_new` in PRQL.
This is because another name is needed to avoid the behavior that
using the column name `date` here would add a new column called `date:1`
instead of updating the original `date` column.

In DuckDB SQL, we can use [Replace Clause](https://duckdb.org/docs/sql/expressions/star#replace-clause)
to update the original column with the same column name.

The SQL generated by the PRQL compiler looks like this:

```{prql}
#| connection: null
#| echo: false
<<prql_cleaning>>
```

## Tidy Data {#sec-tidy-data}

:::{.callout-important}

- PRQL does not yet support PIVOT and UNPIVOT. ([PRQL/prql#644](https://github.com/PRQL/prql/issues/644))
- DuckDB SQL supports PIVOT and UNPIVOT >= 0.8. ([duckdb/duckdb#6387](https://github.com/duckdb/duckdb/pull/6387))

:::

### Unpivot

Transforms the data from wide format to long format.
This transformation is called by names such as unpivot, pivot longer, and melt.

:::{.panel-tabset}

#### PRQL DuckDB

:::{.callout-important}

`games` in this query is defiened in @sec-cleaning with SQL, not with PRQL.

:::

```{prql}
#| filename: PRQL
from s"""
SELECT *
FROM (
  PIVOT_LONGER games
  ON away_team, home_team
  INTO
    NAME variable
    VALUE team
)
"""
group {team} (
  sort this.date
  derive rest = this.date - (this.date | lag 1) - 1
)
select !{away_points, home_points}
filter rest != null
sort game_id
take 5
```

#### SQL DuckDB

```{sql}
--| filename: SQL
--| cache: false
--| output: false
CREATE TABLE tidy AS (
  WITH _tab1 AS (
    PIVOT_LONGER games
    ON away_team, home_team
    INTO
      NAME variable
      VALUE team
  ),

  _tab2 AS (
    SELECT
      COLUMNS(x -> NOT suffix(x, '_points'))
    FROM _tab1
  ),

  _tab3 AS (
    SELECT
      *,
      date - lag(date) OVER (PARTITION BY team ORDER BY date) -1 AS rest
    FROM _tab2
  )

  SELECT *
  FROM _tab3
  WHERE rest IS NOT NULL
  ORDER BY game_id
)
```

```{glue_sql}
#| filename: SQL
FROM tidy
LIMIT 5
```

#### dplyr R

```{r}
#| filename: R
tidy <- games |>
  tidyr::pivot_longer(
    cols = c(away_team, home_team),
    names_to = "variable",
    values_to = "team"
  ) |>
  select(!ends_with("_points")) |>
  arrange(game_id) |>
  mutate(
    rest = date - lag(date) - 1,
    .by = team
  ) |>
  filter(!is.na(rest))
```

```{r}
#| filename: R
tidy |>
  head(5)
```

#### Python Polars

```{python}
#| filename: Python
#| cache: false
tidy = (
    games.melt(
        id_vars=["game_id", "date"],
        value_vars=["away_team", "home_team"],
        value_name="team",
    )
    .sort("game_id")
    .with_columns(
      rest=(pl.col("date").diff().over("team").dt.total_days() - 1).cast(pl.Int8)
    )
    .drop_nulls("rest")
)
```

```{python}
#| filename: Python
tidy.head(5)
```

:::

PRQL, SQL and dplyr remove unnecessary columns after UNPIVOT
(columns that were automatically removed in the original Polars and Pandas example).

### Pivot

Transforms the data from long format to wide format.
This transformation is called by names such as pivot, pivot wider.

:::{.panel-tabset}

#### PRQL DuckDB

:::{.callout-important}

`tidy` in this query is defiened in @sec-tidy-data with SQL,
and `games` is defiened in @sec-cleaning with SQL.

:::

```{prql}
#| filename: PRQL
#| label: prql_tidy_nba_2
from s"""
SELECT *
FROM (
  PIVOT_WIDER tidy ON variable USING FIRST(rest) GROUP BY (game_id, date)
)
"""
derive {
  away_rest = away_team,
  home_rest = home_team
}
select !{
  away_team,
  home_team
}
join side:left games (==game_id && ==date)
derive {
  home_win = games.home_points > games.away_points,
  rest_spread = home_rest - away_rest
}
sort games.game_id
take 5
```

#### SQL DuckDB

```{sql}
--| filename: SQL
--| cache: false
--| output: false
CREATE TABLE by_game AS (
  WITH _tab1 AS (
    PIVOT_WIDER tidy ON variable USING FIRST(rest) GROUP BY (game_id, date)
  )

  SELECT
    * EXCLUDE(away_team, home_team),
    away_team AS away_rest,
    home_team AS home_rest
  FROM _tab1
)
```

```{sql}
--| filename: SQL
--| cache: false
--| output: false
CREATE TABLE joined AS (
  SELECT
    *,
    home_points > away_points AS home_win,
    home_rest - away_rest AS rest_spread
  FROM by_game
  LEFT JOIN games USING (game_id, date)
  ORDER BY game_id
)
```

```{glue_sql}
#| filename: SQL
FROM joined
LIMIT 5
```

#### dplyr R

```{r}
#| filename: R
by_game <- tidy |>
  tidyr::pivot_wider(
    id_cols = c("game_id", "date"),
    values_from = "rest",
    names_from = "variable"
  ) |>
  rename(
    away_rest = away_team,
    home_rest = home_team
  )

joined <- by_game |>
  left_join(games, by = c("game_id", "date")) |>
  mutate(
    home_win = home_points > away_points,
    rest_spread = home_rest - away_rest
  )
```

```{r}
#| filename: R
joined |>
  head(5)
```

#### Python Polars

```{python}
#| filename: Python
by_game = tidy.pivot(
    values="rest", index=["game_id", "date"], columns="variable"
).rename({"away_team": "away_rest", "home_team": "home_rest"})

joined = by_game.join(games, on=["game_id", "date"]).with_columns(
    home_win=pl.col("home_points") > pl.col("away_points"),
    rest_spread=pl.col("home_rest") - pl.col("away_rest"),
)
```

```{python}
#| filename: Python
joined.head(5)
```

:::

There are more columns in the PRQL result than in the other results.
Because the output SQL is not using `USING` for joins ([PRQL/prql#1335](https://github.com/PRQL/prql/issues/1335)).

The SQL generated by the PRQL compiler looks like this:

```{prql}
#| connection: null
#| echo: false
<<prql_tidy_nba_2>>
```
