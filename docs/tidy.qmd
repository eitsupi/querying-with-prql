---
title: Reshaping and Tidy Data
description: Make the data tidy.
engine: knitr
knitr:
  opts_chunk:
    connection: con
    engine-opts:
      target: sql.duckdb
      use_glue: true
execute:
  cache: true
sidebar_position: 4
---

:::{.callout-note}

This page is based on the chapter ["Reshaping and Tidy Data"](https://kevinheavey.github.io/modern-polars/tidy.html)
of the Modern Polars book.

:::

{{< include _setup-knitr.qmd >}}

## Read the data

Download the website tables as CSV files.
I use R to download the date here, but we can also download and use the CSV files included in
[the `kevinheavey/modern-polars` GitHub repository](https://github.com/kevinheavey/modern-polars/tree/master/data/nba).

```{r}
#| code-fold: true
#| cache: false
nba_dir <- file.path("data", "nba")

column_names <- c(
  date = "date",
  away_team = "visitor_neutral",
  away_points = "pts",
  home_team = "home_neutral",
  home_points = "pts_2"
)

.write_data <- function(month) {
  base_url <- "http://www.basketball-reference.com/leagues/NBA_2016_games-{month}.html"

  glue::glue(base_url, month = month) |>
    rvest::read_html() |>
    rvest::html_table() |>
    (\(x) x[[1]])() |> # TODO: Rewrite after R 4.3
    janitor::clean_names() |>
    dplyr::select(all_of(column_names)) |>
    dplyr::filter(date != "Playoffs") |>
    readr::write_csv(file.path(nba_dir, glue::glue("{month}.csv")))
}

if (!fs::dir_exists(nba_dir)) {
  fs::dir_create(nba_dir)
  months <- c(
    "october",
    "november",
    "december",
    "january",
    "february",
    "march",
    "april",
    "may",
    "june"
  )

  months |>
    purrr::walk(.write_data)
}
```

After the CSV files are ready, load these into DuckDB (in-memory) database table,
R DataFrame, and Python polars.LazyFrame.

:::{.panel-tabset}

### DuckDB

```{r }
#| cache: false
#| output: false
#| echo: false
con <- DBI::dbConnect(duckdb::duckdb(), ":memory:")
```

```{glue_sql}
#| cache: false
#| output: false
CREATE TABLE tab AS FROM read_csv_auto('data/nba/*.csv')
```

```{glue_sql}
FROM tab
LIMIT 5
```

### R DataFrame

```{r}
#| cache: false
library(dplyr, warn.conflicts = FALSE)

df <- readr::read_csv(
  fs::dir_ls("data/nba", glob = "*.csv")
)
```

```{r}
df |>
  head(5)
```

### Python polars.LazyFrame

```{python}
#| cache: false
import polars as pl

lf = pl.scan_csv("data/nba/*.csv")
```

```{python}
lf.head(5).collect()
```

:::

## Cleaning {#sec-cleaning}

Convert the `date` column to date type and delete rows containing missing values (`null`).

PRQL does not have a "remove rows with missing values in any column" syntax
([PRQL/prql#2386](https://github.com/PRQL/prql/issues/2386)),
but DuckDB SQL does (>= 0.8, [duckdb/duckdb#6621](https://github.com/duckdb/duckdb/pull/6621)), so it can be used.

:::{.panel-tabset}

### PRQL DuckDB

```{prql, prql_cleaning}
let games = (
  from tab
  filter s"COLUMNS(*) IS NOT NULL"
  derive date_new = (s"strptime(date, '%a, %b %d, %Y')" | as date)
  select ![_frame.date] # https://github.com/PRQL/prql/issues/2155
  sort date_new
  derive game_id = row_number
)

from games
take 5
```

### SQL DuckDB

```{sql}
--| cache: false
--| output: false
CREATE TABLE games AS (
  WITH _tab1 AS (
      SELECT
        * REPLACE (strptime(date, '%a, %b %d, %Y')::date AS date)
      FROM tab
      WHERE COLUMNS(*) IS NOT NULL
    )

  SELECT
    row_number() OVER(ORDER BY date) AS game_id,
    *
  FROM _tab1
  ORDER BY date
)
```

```{glue_sql}
FROM games
LIMIT 5
```

### dplyr R

```{r}
games <- df |>
  filter(!if_any(everything(), is.na)) |>
  mutate(
    date = lubridate::parse_date_time(date, "%a, %b %d, %Y") |>
      lubridate::as_date()
  ) |>
  arrange(date) |>
  mutate(game_id = row_number(), .before = 1)
```

```{r}
games |>
  head(5)
```

### Python Polars

```{python}
#| cache: false
games = (
    lf.filter(~pl.all(pl.all().is_null()))
    .with_columns(
        pl.col("date").str.strptime(pl.Date, "%a, %b %d, %Y"),
    )
    .sort("date")
    .with_row_count("game_id")
    .collect()
)
```

```{python}
games.head(5)
```

:::

Looking at the result tables, we notice that the PRQL result is different from the other results;
A column named `date` in other results is named `date_new` in PRQL.
This is because another name is needed to avoid the behavior that
using the column name `date` here would add a new column called `date:1`
instead of updating the original `date` column.

In DuckDB SQL, we can use [Replace Clause](https://duckdb.org/docs/sql/expressions/star#replace-clause)
to update the original column with the same column name.

The SQL generated by the PRQL compiler looks like this:

```{prql}
#| connection: null
#| echo: false
<<prql_cleaning>>
```

## Tidy Data {#sec-tidy-data}

:::{.callout-important}

- PRQL does not yet support PIVOT and UNPIVOT. ([PRQL/prql#644](https://github.com/PRQL/prql/issues/644))
- DuckDB SQL supports PIVOT and UNPIVOT >= 0.8. ([duckdb/duckdb#6387](https://github.com/duckdb/duckdb/pull/6387))

:::

### Unpivot

Transforms the data from wide format to long format.
This transformation is called by names such as unpivot, pivot longer, and melt.

:::{.panel-tabset}

#### PRQL DuckDB

:::{.callout-important}

`games` in this query is defiened in @sec-cleaning with SQL, not with PRQL.

:::

```{prql}
from s"""
SELECT *
FROM (
  PIVOT_LONGER games
  ON away_team, home_team
  INTO
    NAME variable
    VALUE team
)
"""
group [team] (
  sort _frame.date
  derive rest = _frame.date - (_frame.date | lag 1) - 1
)
filter rest != null
sort game_id
take 5
```

#### SQL DuckDB

```{sql}
--| cache: false
--| output: false
CREATE TABLE tidy AS (
  WITH _tab1 AS (
    PIVOT_LONGER games
    ON away_team, home_team
    INTO
      NAME variable
      VALUE team
  ),

  _tab2 AS (
    SELECT
      COLUMNS(x -> NOT suffix(x, '_points'))
    FROM _tab1
  ),

  _tab3 AS (
    SELECT
      *,
      date - lag(date) OVER (PARTITION BY team ORDER BY date) -1 AS rest
    FROM _tab2
  )

  SELECT *
  FROM _tab3
  WHERE rest IS NOT NULL
  ORDER BY game_id
)
```

```{glue_sql}
FROM tidy
LIMIT 5
```

#### dplyr R

```{r}
tidy <- games |>
  tidyr::pivot_longer(
    cols = c(away_team, home_team),
    names_to = "variable",
    values_to = "team"
  ) |>
  select(!ends_with("_points")) |>
  arrange(game_id) |>
  mutate(
    rest = date - lag(date) - 1,
    .by = team
  ) |>
  filter(!is.na(rest))
```

```{r}
tidy |>
  head(5)
```

#### Python Polars

```{python}
#| cache: false
tidy = (
    games.melt(
        id_vars=["game_id", "date"],
        value_vars=["away_team", "home_team"],
        value_name="team",
    )
    .sort("game_id")
    .with_columns(rest=(pl.col("date").diff().over("team").dt.days() - 1).cast(pl.Int8))
    .drop_nulls("rest")
)
```

```{python}
tidy.head(5)
```

:::

SQL and dplyr remove unnecessary columns after UNPIVOT
(columns that were automatically removed in the original Polars and Pandas example),
but PRQL result have more columns than the other results because PRQL could not remove columns ([PRQL/prql#2398](https://github.com/PRQL/prql/issues/2398)).

### Pivot

Transforms the data from long format to wide format.
This transformation is called by names such as pivot, pivot wider.

:::{.panel-tabset}

#### PRQL DuckDB

:::{.callout-important}

`tidy` in this query is defiened in @sec-tidy-data with SQL,
and `games` is defiened in @sec-cleaning with SQL.

:::

```{prql prql_tidy_nba_2}
from s"""
SELECT *
FROM (
  PIVOT_WIDER tidy ON variable USING FIRST(rest) GROUP BY (game_id, date)
)
"""
derive [
  away_rest = away_team,
  home_rest = home_team
]
join side:left games [==game_id, ==date]
derive [
  home_win = games.home_points > games.away_points,
  rest_spread = home_rest - away_rest
]
sort games.game_id
take 5
```

#### SQL DuckDB

```{sql}
--| cache: false
--| output: false
CREATE TABLE by_game AS (
  WITH _tab1 AS (
    PIVOT_WIDER tidy ON variable USING FIRST(rest) GROUP BY (game_id, date)
  )

  SELECT
    * EXCLUDE(away_team, home_team),
    away_team AS away_rest,
    home_team AS home_rest
  FROM _tab1
)
```

```{sql}
--| cache: false
--| output: false
CREATE TABLE joined AS (
  SELECT
    *,
    home_points > away_points AS home_win,
    home_rest - away_rest AS rest_spread
  FROM by_game
  LEFT JOIN games USING (game_id, date)
  ORDER BY game_id
)
```

```{glue_sql}
FROM joined
LIMIT 5
```

#### dplyr R

```{r}
by_game <- tidy |>
  tidyr::pivot_wider(
    id_cols = c("game_id", "date"),
    values_from = "rest",
    names_from = "variable"
  ) |>
  rename(
    away_rest = away_team,
    home_rest = home_team
  )

joined <- by_game |>
  left_join(games, by = c("game_id", "date")) |>
  mutate(
    home_win = home_points > away_points,
    rest_spread = home_rest - away_rest
  )
```

```{r}
joined |>
  head(5)
```

#### Python Polars

```{python}
by_game = tidy.pivot(
    values="rest", index=["game_id", "date"], columns="variable"
).rename({"away_team": "away_rest", "home_team": "home_rest"})

joined = by_game.join(games, on=["game_id", "date"]).with_columns(
    home_win=pl.col("home_points") > pl.col("away_points"),
    rest_spread=pl.col("home_rest") - pl.col("away_rest"),
)
```

```{python}
joined.head(5)
```

:::

Again, there are more columns in the PRQL result than in the other results.
A bug prevented me from deleting unnecessary columns ([PRQL/prql#2398](https://github.com/PRQL/prql/issues/2398))
and the output SQL is not using `USING` for joins ([PRQL/prql#1335](https://github.com/PRQL/prql/issues/1335)).

The SQL generated by the PRQL compiler looks like this:

```{prql}
#| connection: null
#| echo: false
<<prql_tidy_nba_2>>
```
